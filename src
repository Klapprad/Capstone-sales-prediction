## Headder imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import learning_curve
import warnings
warnings.filterwarnings('ignore')



# Source
train_df = pd.read_csv('train.csv')
store_df = pd.read_csv('store.csv')


#print(train_df.info())
#print(train_df.head())
#print(store_df.info())
#print(store_df.head())

#print(train_df.isnull().sum())
#print(store_df.isnull().sum())

###########################################################Datenbereinigung
## Date als datetime formatieren
train_df['Date'] = pd.to_datetime(train_df['Date'])

## String der holiday als Variable speichern
train_df['StateHoliday'] = train_df['StateHoliday'].astype('category')

## fehlende Werte mit Median auffüllen
store_df['CompetitionDistance'] = store_df['CompetitionDistance'].fillna(
    store_df['CompetitionDistance'].median())

# Zeitangaben bei cometitors -> wenn keine Angabe, dann keine konkurrenz
store_df['CompetitionOpenSinceMonth'] = store_df['CompetitionOpenSinceMonth'].fillna(0)
store_df['CompetitionOpenSinceYear'] = store_df['CompetitionOpenSinceYear'].fillna(0)

### hier genauso, wenn keine Daten dann gab es auch keine Promo2
store_df['Promo2SinceWeek'] = store_df['Promo2SinceWeek'].fillna(0)
store_df['Promo2SinceYear'] = store_df['Promo2SinceYear'].fillna(0)
store_df['PromoInterval'] = store_df['PromoInterval'].fillna('None')

## Asugabe der Daten -> fallen Ausreißer oder unplausible auf?
#print("\nInfo Train:")
#print(train_df.info())
#print("\ninfo store:")
#print(store_df.info())
#print("\nStatistik ausgeben:")
#print(train_df.describe())

####woche und jahr aus Datum extrahieren (wir wollen ja wöchentliche Vorhersagen machen)...
train_df['Year'] = train_df['Date'].dt.year
train_df['Week'] = train_df['Date'].dt.isocalendar().week

# ... und wöchentliche Aggregation
weekly_sales = train_df.groupby(['Store', 'Year', 'Week']).agg({
    'Sales': 'sum',
    'Customers': 'sum',
    'Open': 'sum',
    'Promo': 'mean',
    'SchoolHoliday': 'mean'
}).reset_index()

# wöchentliche Daten ausgeben
weekly_sales['AvgDailySales'] = weekly_sales['Sales'] / weekly_sales['Open']
weekly_sales['AvgDailyCustomers'] = weekly_sales['Customers'] / weekly_sales['Open']
weekly_sales['SalesPerCustomer'] = weekly_sales['Sales'] / weekly_sales['Customers']

print("\nStats zu wöchentlichen Daten")
print(weekly_sales.describe())
print(weekly_sales.groupby('Store').size().describe())
print(weekly_sales.head())


################################################# Erste aufbereitung und Visualisirung
### Plotting
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

## Mean der wöchentlichen Sales
weekly_avg_sales = weekly_sales.groupby(['Year', 'Week'])['Sales'].mean().reset_index()
ax1.plot(range(len(weekly_avg_sales)), weekly_avg_sales['Sales'])
ax1.set_title('Wöchentliche Verkäufe (avg)')
ax1.set_xlabel('Wochen')
ax1.set_ylabel('Verkäufe')

# verkäufe pro Kunde
ax2.hist(weekly_sales['SalesPerCustomer'].dropna(), bins=30)
ax2.set_title('Verkäufe vs. Kunden')
ax2.set_xlabel('Anzahl Kunden')
ax2.set_ylabel('Häufigkeit')

# Mean Verkäufe nach Wochentag
daily_sales = train_df.groupby('DayOfWeek')['Sales'].mean()
ax3.bar(daily_sales.index, daily_sales.values)
ax3.set_title('Verkäufe nach Wochentag')
ax3.set_xlabel('Wochentag')
ax3.set_ylabel('Durchschnittliche Verkäufe')

#verkäufe vs. Kunden
ax4.scatter(weekly_sales['Customers'], weekly_sales['Sales'], alpha=0.1)
ax4.set_title('Verkäufe pro Kunde (€)')
ax4.set_xlabel('Anzahl Kunden')
ax4.set_ylabel('Verkäufe')

plt.tight_layout()
plt.show()

print("\nKennzahlen:")
summary_stats = weekly_sales[['Sales', 'Customers', 'SalesPerCustomer']].describe().round(2)
print(summary_stats)

########################################### Performance der Ladenmodelle

# Targets festlegen, es geht ja um die Verkäufe -> Store-Daten mit Verkaufsdaten verbinden
store_performance = weekly_sales.merge(store_df[['Store', 'StoreType', 'Assortment']], on='Store')

### plotting
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))

#
store_type_sales = store_performance.groupby('StoreType')['Sales'].mean().sort_values(ascending=False)
ax1.bar(store_type_sales.index, store_type_sales.values)
ax1.set_title('Verkäufe nach Filialtyp')
ax1.set_ylabel('Durchschnittliche Verkäufe')

#
store_type_spc = store_performance.groupby('StoreType')['SalesPerCustomer'].mean().sort_values(ascending=False)
ax2.bar(store_type_spc.index, store_type_spc.values)
ax2.set_title('Verkäufe pro Kunde nach Filialtyp')
ax2.set_ylabel('€ pro Kunde')

#
promo_effect = store_performance.groupby(['StoreType', 'Promo'])['Sales'].mean().unstack()
promo_effect['Steigerung_%'] = ((promo_effect[1] - promo_effect[0]) / promo_effect[0] * 100).round(1)
ax3.bar(promo_effect.index, promo_effect['Steigerung_%'])
ax3.set_title('Promotions-Effekte nach Filialtyp')
ax3.set_ylabel('Verkaufssteigerung in %')

#
store_type_customers = store_performance.groupby('StoreType')['Customers'].mean().sort_values(ascending=False)
ax4.bar(store_type_customers.index, store_type_customers.values)
ax4.set_title('Kundenanzahl nach Filialtyp (avg)')
ax4.set_ylabel('Kundenanzahl')

plt.tight_layout()
plt.show()

print("\nZusammenfassung nach Filialtyp:")
summary_stores = store_performance.groupby('StoreType').agg({
    'Sales': ['mean', 'std'],
    'Customers': 'mean',
    'SalesPerCustomer': 'mean',
    'Promo': 'mean'
}).round(2)
print(summary_stores)


########################## Analyse der Promotionseffekte

fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 6))

###Verkäufe mit/ohne Promotion und Filialtyp
promo_effect = store_performance.groupby(['StoreType', 'Promo'])['Sales'].mean().unstack()
width = 0.35
x = np.arange(len(promo_effect.index))

ax1.bar(x - width/2, promo_effect[0], width, label='Ohne Promo')
ax1.bar(x + width/2, promo_effect[1], width, label='Mit Promo')
ax1.set_title('Verkäufe mit/ohne Promotion nach Filialtyp')
ax1.set_xticks(x)
ax1.set_xticklabels(promo_effect.index)
ax1.set_ylabel('Durchschnittliche Verkäufe (€)')
ax1.legend()

#
promo_increase = ((promo_effect[1] - promo_effect[0]) / promo_effect[0] * 100).round(1)
ax2.bar(promo_effect.index, promo_increase)
ax2.set_title('Verkaufssteigerung durch Promotion')
ax2.set_ylabel('Steigerung in %')

plt.tight_layout()
plt.show()

### detaillierte Ausgabe
print("\nPromotionseffekte nach Filialtyp:")
for store_type in store_performance['StoreType'].unique():
    type_data = store_performance[store_performance['StoreType'] == store_type]
    no_promo = type_data[type_data['Promo'] == 0]['Sales'].mean()
    with_promo = type_data[type_data['Promo'] == 1]['Sales'].mean()
    increase = ((with_promo - no_promo) / no_promo * 100).round(1)

    print(f"\nFilialtyp {store_type}:")
    print(f"Ohne Promotion: {no_promo:.2f}€")
    print(f"Mit Promotion: {with_promo:.2f}€")
    print(f"Steigerung: {increase}%")

    # Kundenverhalten währedn Promotions
type_b_analysis = store_performance[store_performance['StoreType'] == 'b'].groupby('Promo').agg({
    'Sales': ['mean', 'std'],
    'Customers': ['mean', 'count'],
    'SalesPerCustomer': 'mean'
}).round(2)

# Vergleich Kundenfrequenz vs Durchschnittskäufe
print("Detailanalyse Typ-B-Filialen während Promotionen:")
print(type_b_analysis)

# Promotionseffekte nach Wochentag
weekly_promo_effect = store_performance[store_performance['StoreType'] == 'b'].groupby(['DayOfWeek', 'Promo']).agg({
    'Sales': 'mean',
    'Customers': 'mean'
}).round(2)

print("\nPromotionseffekte nach Wochentag für Typ-B:")
print(weekly_promo_effect)


################################################## Heatmap als Entscheidungshilfe für erste Featureasuwahl

def heatmap(df):

    #numerische Spalten
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns

    #Heatmap
    corr_matrix = df[numeric_cols].corr()

    plt.figure(figsize=(12, 8))

    sns.heatmap(corr_matrix,
                annot=True,  
                cmap='coolwarm',  
                center=0,  
                fmt='.2f',  
                square=True, 
                linewidths=0.5)  

    plt.title('Korrelationsmatrix der numerischen Features', pad=20)
    plt.tight_layout()
    plt.show()

    sales_corr = corr_matrix['Sales'].sort_values(ascending=False)  ##ausgabe der korrelation aufdie Sales
    print("\nKorrelationen mit Sales:")
    print(sales_corr)

heatmap(train_df)


################################################Feature Engineering Basic


# TRain und Testdaten mergen
train_df = train_df.merge(store_df, on='Store', how='left')

# zeitliche Features auswählen
# wir haben oben bei der EDA einen Zeitlichen zusammenhang gesehen
train_df['Date'] = pd.to_datetime(train_df['Date'])  #datum in datetime
train_df['Year'] = train_df['Date'].dt.year          
train_df['Month'] = train_df['Date'].dt.month        
train_df['Week'] = train_df['Date'].dt.isocalendar().week  

# Store-Typen in Zahlen umwandeln
# verschiedene Ladenmodelle perfomen anders
train_df['StoreType_Encoded'] = train_df['StoreType'].map({
    'a': 0,
    'b': 1,
    'c': 2,
    'd': 3
})

# Gleiches für Sortiment (kann eigentlich weg)
train_df['Assortment_Encoded'] = train_df['Assortment'].map({
    'a': 0,
    'b': 1,
    'c': 2
})

# weitere Features erstellen
train_df['IsWeekend'] = (train_df['DayOfWeek'].isin([6,7])).astype(int)

train_df['IsHighSeason'] = (train_df['Month'].isin([11,12])).astype(int)   ###HighSeason = November/Dezemebr

# Wie weit/nah ist die Konkurrenz ansässig? (kleiner als Median der Konkurrenzdistanz)
median_distance = train_df['CompetitionDistance'].median()
train_df['HasNearCompetition'] = (train_df['CompetitionDistance'] < median_distance).astype(int)

# Promotion Features
# Kombination aus Promo und Wochenende
train_df['Promo_Weekend'] = train_df['Promo'] * train_df['IsWeekend']

# Featureliste erstellen
selected_features = [
    'Store',
    'DayOfWeek',
    'Promo',
    'SchoolHoliday',
    'StoreType_Encoded',
    'Assortment_Encoded',
    'CompetitionDistance',
    'IsWeekend',
    'HasNearCompetition',
    'IsHighSeason',
    'Month',
    'Year',
    'Promo_Weekend',
    'Customers',                                    # Erweitert um Customers (sollte auch in Feature importance dabei sein)
    'Sales_per_Customer'                           # 
]


################################################################################################## Features Überprüfen

#Zusammenhang Promo, DayOfWeek und Sales 
print("\nDurchschnittliche Sales nach Wochentag und Promo:")
avg_sales = train_df.groupby(['DayOfWeek', 'Promo'])['Sales'].mean().unstack()
print(avg_sales)

#
plt.figure(figsize=(12, 6))
avg_sales.plot(kind='bar', width=0.8)
plt.title('Durchschnittliche Sales nach Wochentag und Promo')
plt.xlabel('Tag der Woche')
plt.ylabel('Durchschnittliche Sales')
plt.legend(['Keine Promo', 'Promo'])
plt.tight_layout()
plt.show()

# nochmal heatmap jetzt mit wichtigsten Features (jetzt mit StoreType)
correlation_features = ['Sales', 'DayOfWeek', 'Promo', 'CompetitionDistance',
                       'StoreType_Encoded', 'Month']
correlation_matrix = train_df[correlation_features].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Korrelationsmatrix der wichtigsten Features')
plt.tight_layout()
plt.show()

############################################################################ 
#####Erweitertes Feature Engineering um Vorhersagen zu verbessernb


# Features von oben
train_df['Promo_DayOfWeek'] = train_df['Promo'] * train_df['DayOfWeek']
store_avg_sales = train_df.groupby('Store')['Sales'].mean()
train_df['Store_Avg_Sales'] = train_df['Store'].map(store_avg_sales)
dow_avg_sales = train_df.groupby('DayOfWeek')['Sales'].mean()
train_df['DayOfWeek_Avg_Sales'] = train_df['DayOfWeek'].map(dow_avg_sales)

# zeitbasierte Features

# sortieren nach Store und Datum
train_df = train_df.sort_values(['Store', 'Year', 'Month', 'DayOfWeek'])

# Verkäufe der letzten 7 Tage
train_df['Sales_Lag7'] = train_df.groupby('Store')['Sales'].shift(7)
## fehlende Werte mit Durchschnitt Stores
train_df['Sales_Lag7'].fillna(train_df['Store_Avg_Sales'], inplace=True)
# und letzte 7 bzw 14 Tage
train_df['Sales_MA7'] = train_df.groupby('Store')['Sales'].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean()
)
train_df['Sales_MA14'] = train_df.groupby('Store')['Sales'].transform(
    lambda x: x.rolling(window=14, min_periods=1).mean()
)

## Veränderung zum gleichen Tag der Vorwoche
train_df['Sales_RelativeChange'] = (train_df['Sales'] - train_df['Sales_Lag7']) / train_df['Sales_Lag7']
train_df['Sales_RelativeChange'].replace([np.inf, -np.inf], 0, inplace=True)
train_df['Sales_RelativeChange'].fillna(0, inplace=True)

# Featureliste erweitern um:
selected_features.extend([
    'Promo_DayOfWeek',
    'Store_Avg_Sales',
    'DayOfWeek_Avg_Sales',
    'Sales_Lag7',
    'Sales_MA7',
    'Sales_MA14',
    'Sales_RelativeChange'
])

# Features und Target definieren
X = train_df[selected_features]    ##features
y = train_df['Sales']              ##target

# Split 80:20
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

def plot_feature_importance(model, X_train):

    importances = model.feature_importances_
    feature_names = X_train.columns
    indices = np.argsort(importances)[::-1]

    plt.figure(figsize=(12, 6))
    plt.title('Feature Importance mit Customers', pad=20)
    plt.bar(range(X_train.shape[1]), importances[indices])
    plt.xticks(range(X_train.shape[1]), [feature_names[i] for i in indices], rotation=45, ha='right')
    plt.tight_layout()
    plt.show()


    importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    })
    importance_df = importance_df.sort_values('Importance', ascending=False)
    print("\nFeature Importance Ranking:")
    print(importance_df.to_string(index=False))


# nur für Radnom Forest
rf_model = results['Random Forest']['model']

plot_feature_importance(rf_model, X_train)

####################################################################### MODELLING 

X = train_df[selected_features]  # von Featuresliste
y = train_df['Sales']           # Zielvariable

# Split 80:20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Modelloptimierung händisch durchprobieren und nicht via GridSearch
models = {
    'Linear Regression': LinearRegression(
    ),
    'Random Forest': RandomForestRegressor(
        n_estimators=100,      # anzahl Bäume -> 100 bis 200
        max_depth=10,         # tiefe
        min_samples_leaf=2,   # Mindestanzahl von Samples pro Blattn 
        random_state=42
    ),
    'Gradient Boosting': GradientBoostingRegressor(
        n_estimators=100,      # Bäume
        learning_rate=0.1,    # Lernrate = korrektur vorheriger fehler
        max_depth=5,          # nicht so tief Rüdiger
        min_samples_leaf=2,   # Samples pro Blatt
        random_state=42
    )
}

results = {}

# wir vergleichen 3 Modelle...
for name, model in models.items():

    model.fit(X_train, y_train)

    train_predictions = model.predict(X_train)
    test_predictions = model.predict(X_test)

    # Perfomance berenchne
    train_rmse = np.sqrt(mean_squared_error(y_train, train_predictions))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions))
    train_r2 = r2_score(y_train, train_predictions)
    test_r2 = r2_score(y_test, test_predictions)
    train_mae = mean_absolute_error(y_train, train_predictions)
    test_mae = mean_absolute_error(y_test, test_predictions)

    results[name] = {
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_mae': train_mae,
        'test_mae': test_mae,
        'model': model
    }

    print(f"{name} Performance:")
    print(f"Training RMSE: {train_rmse:.2f}")
    print(f"Test RMSE: {test_rmse:.2f}")
    print(f"Training R²: {train_r2:.2f}")
    print(f"Test R²: {test_r2:.2f}")
    print(f"Training MAE: {train_mae:.2f}")
    print(f"Test MAE: {test_mae:.2f}")


print("\nModellvergleich:")
for name, metrics in results.items():
    print(f"\n{name}:")
    print(f"Train RMSE: {metrics['train_rmse']:.2f}, Test RMSE: {metrics['test_rmse']:.2f}")
    print(f"Train R²: {metrics['train_r2']:.2f}, Test R²: {metrics['test_r2']:.2f}")
    print(f"Train MAE: {metrics['train_mae']:.2f}, Test MAE: {metrics['test_mae']:.2f}")

    ############################# METRIKEN Overfitting prüfen

print("\nOverfittinganalyse")

train_rmse = np.sqrt(mean_squared_error(y_train, models['Random Forest'].predict(X_train)))
test_rmse = np.sqrt(mean_squared_error(y_test, models['Random Forest'].predict(X_test)))
train_r2 = r2_score(y_train, models['Random Forest'].predict(X_train))
test_r2 = r2_score(y_test, models['Random Forest'].predict(X_test))

## delta zwischen train und test:
rmse_diff = abs(train_rmse - test_rmse)
r2_diff = abs(train_r2 - test_r2)

print(f"RMSE-Differenz (Train vs Test): {rmse_diff:.2f}")
print(f"R²-Differenz (Train vs Test): {r2_diff:.4f}") 

## im Idealfall möglichst klein weist auf gute Generalisierung hin

### Overfitting-Analyse für Random Forest ###

#Differenz-Analyse:
#RMSE-Differenz (Train-Test): 4.20
#R²-Differenz (Train-Test): 0.0003
